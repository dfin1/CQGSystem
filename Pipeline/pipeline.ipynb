{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10e24b3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b45b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dfingerlos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO) # instead of .INFO\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.special import softmax\n",
    "from web_search import get_html, WebParser, duckduckgo_search\n",
    "from flask import request, Flask, jsonify\n",
    "from flask_cors import CORS\n",
    "from scipy import spatial\n",
    "from faspect import Faspect\n",
    "from flask import request, Flask\n",
    "from flask_cors import CORS, cross_origin\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import argparse\n",
    "import webbrowser\n",
    "from datetime import datetime, timedelta\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score\n",
    "from sentence_transformers.cross_encoder import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472a26b",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "697834c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snippetsCount = 20 #Fascet extraction is trained on snippetcount 20\n",
    "snippetsBatchCnt = 4\n",
    "modelpath = \"hf_v2_6_Model_20f.ckpt\" # \"../hf_v1_0_Model.ckpt\" #\n",
    "fascetCount4Generation = 20\n",
    "fascetThreshold = 0.80\n",
    "CEmodel_path = \"../output/training_F0_7-2022-06-20_16-30-36\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264ee21",
   "metadata": {},
   "source": [
    "### Webcrawle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0781d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quin:\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "\n",
    "        self.sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.sent_tokenizer._params.abbrev_types.update(['e.g', 'i.e', 'subsp'])\n",
    "\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "\n",
    "        logging.info('Initialized!')\n",
    "\n",
    "    def extract_snippets(self, text, sentences_per_snippet=4): #how many sentences in every snippet\n",
    "        sentences = self.sent_tokenizer.tokenize(text)\n",
    "        snippets = []\n",
    "        i = 0\n",
    "        last_index = 0\n",
    "        while i < len(sentences):\n",
    "            snippet = ' '.join(sentences[i:i + sentences_per_snippet])\n",
    "            if len(snippet.split(' ')) > 4:\n",
    "                snippets.append(snippet)\n",
    "            last_index = i + sentences_per_snippet\n",
    "            i += sentences_per_snippet\n",
    "        if last_index < len(sentences):\n",
    "            snippet = ' '.join(sentences[last_index:])\n",
    "            if len(snippet.split(' ')) > 4:\n",
    "                snippets.append(snippet)\n",
    "        return snippets\n",
    "\n",
    "    def search_web_evidence(self, query):\n",
    "        logging.info('searching the web...')\n",
    "        urls = duckduckgo_search(query, pages=2)\n",
    "        logging.info('downloading {} web pages...'.format(len(urls)))\n",
    "        search_results = []\n",
    "\n",
    "        def download(url):\n",
    "            nonlocal search_results\n",
    "            data = get_html(url)\n",
    "            soup = BeautifulSoup(data, features='lxml')\n",
    "            title = soup.title.string\n",
    "            w = WebParser()\n",
    "            w.feed(data)\n",
    "            new_snippets = sum([self.extract_snippets(b) for b in w.blocks if b.count(' ') > 20], [])\n",
    "            new_snippets = [{'snippet': p, 'url': url, 'title': title} for p in new_snippets]\n",
    "            search_results += new_snippets\n",
    "\n",
    "        def timeout_download(arg):\n",
    "            pool = ThreadPool(1)\n",
    "            try:\n",
    "                pool.apply_async(download, [arg]).get(timeout=5)\n",
    "            except:\n",
    "                pass\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "        p = ThreadPool(32)\n",
    "        p.map(timeout_download, urls)\n",
    "        p.close()\n",
    "        p.join()\n",
    "\n",
    "        snippets = [s['snippet'] for s in search_results]\n",
    "        logging.info('done searching')\n",
    "\n",
    "        return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd17fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8Decoder(inputStr): #decode the input string \n",
    "    outputStr = inputStr.encode('latin1', errors='ignore').decode('unicode-escape', errors='ignore').encode('latin1', errors='ignore').decode('utf8', errors='ignore')\n",
    "    return outputStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d381f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructSnippets(res): #take entire response from the webcrawler and construct the snippets according to count and batchsize specified under parameters\n",
    "    snippets = []\n",
    "    stepscnt = min(int(snippetsCount), len(res))\n",
    "\n",
    "    for i in range(snippetsBatchCnt):\n",
    "        snippetsEpoch = []\n",
    "        for j in range(stepscnt):\n",
    "            snippetsEpoch.append(utf8Decoder(res[j]))\n",
    "            \n",
    "        res = res[snippetsCount:]\n",
    "        snippets.append(snippetsEpoch)\n",
    "        \n",
    "        #If there are to view entries in res to construct the next Batch fully => stop\n",
    "        if len(res) < snippetsCount:\n",
    "            return snippets\n",
    "        \n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34c672",
   "metadata": {},
   "source": [
    "### Generete Clarifying Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ce0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "  # Instantiate the model\n",
    "  def __init__(self, learning_rate, tokenizer, model, hparams):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.model = model\n",
    "    self.learning_rate = learning_rate\n",
    "    # self.freeze_encoder = freeze_encoder\n",
    "    # self.freeze_embeds_ = freeze_embeds\n",
    "    self.save_hyperparameters(hparams)\n",
    "\n",
    "    if self.hparams.freeze_encoder:\n",
    "      freeze_params(self.model.get_encoder())\n",
    "\n",
    "    if self.hparams.freeze_embeds:\n",
    "      self.freeze_embeds()\n",
    "  \n",
    "  def freeze_embeds(self):\n",
    "    ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n",
    "    freeze_params(self.model.model.shared)\n",
    "    for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "      freeze_params(d.embed_positions)\n",
    "      freeze_params(d.embed_tokens)\n",
    "\n",
    "  # Do a forward pass through the model\n",
    "  def forward(self, input_ids, **kwargs):\n",
    "    return self.model(input_ids, **kwargs)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # Load the data into variables\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "    # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "    # Create the loss function\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    # Calculate the loss on the un-shifted tokens\n",
    "    loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss':loss}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss': val_loss}\n",
    "  \n",
    "  # Method that generates text using the BartForConditionalGeneration's generate() method\n",
    "  def generate_text(self, text, eval_beams, early_stopping = True, max_len = 40):\n",
    "    ''' Function to generate text '''\n",
    "    generated_ids = self.model.generate(\n",
    "        text[\"input_ids\"],\n",
    "        attention_mask=text[\"attention_mask\"],\n",
    "        use_cache=True,\n",
    "        decoder_start_token_id = self.tokenizer.pad_token_id,\n",
    "        num_beams= eval_beams,\n",
    "        max_length = max_len,\n",
    "        early_stopping = early_stopping\n",
    "    )\n",
    "    return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
    "\n",
    "def freeze_params(model):\n",
    "  ''' Function that takes a model as input (or part of a model) and freezes the layers for faster training\n",
    "      adapted from finetune.py '''\n",
    "  for layer in model.parameters():\n",
    "    layer.requires_grade = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8d2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hparams dictionary to pass in the model\n",
    "hparams = argparse.Namespace()\n",
    "hparams.freeze_encoder = True\n",
    "hparams.freeze_embeds = True\n",
    "hparams.eval_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d52580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BartConfig\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bdbe51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(seed_line, model_): #Cenerate Clarifying question\n",
    "    model_.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model_.eval()\n",
    "    prompt_line_tokens = tokenizer(seed_line, max_length = 192, return_tensors = \"pt\", truncation = True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    line = model_.generate_text(prompt_line_tokens, eval_beams = 8)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2eeaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = LitModel.load_from_checkpoint(modelpath , learning_rate = 2e-5, \n",
    "                                             tokenizer = tokenizer, model = bart_model, hparams = hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ae263",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Selecting best Fascets for clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61259ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestFascets(clariQ, fascets, threshold = 0.75):\n",
    "    data = []\n",
    "    for fascet in fascets:\n",
    "        data.append([ clariQ, fascet ])\n",
    "    \n",
    "    #Use CrossEncoder Prediction\n",
    "    CEmodel = CrossEncoder(CEmodel_path)\n",
    "    scores = CEmodel.predict(data)\n",
    "    \n",
    "    #Convert data to pandas Dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    df['scores'] = pd.DataFrame(scores)\n",
    "    df = df.sort_values(by=['scores'], ascending=False)\n",
    "    \n",
    "    #Gen strign with all fascets\n",
    "    FascetStr = \"\"\n",
    "    for row in df.iterrows():\n",
    "        FascetStr = FascetStr + row[1][1] + \" (\" + \"{0:.3f}\".format(row[1]['scores']) + \"), \"\n",
    "    \n",
    "    #select best fascets\n",
    "    fascet_list = df[df.scores > threshold][1].to_list()\n",
    "    \n",
    "    return [(' , '.join(fascet_list)), FascetStr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b56db",
   "metadata": {},
   "source": [
    "### Fascet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a4682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingFascets(fascets, query):\n",
    "    #lemmatize input ?!!?!??! Adapt, just if there is lemmartized 2 times the same word remove it!!\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    for idx, el in enumerate(fascets):\n",
    "        fascets[idx] = lemma.lemmatize(el)\n",
    "    \n",
    "    #remove query as fascet from fascets\n",
    "    for idx, el in enumerate(fascets):\n",
    "        if str(el).lower() == str(query).lower():\n",
    "            fascets.remove(el)\n",
    "        \n",
    "    #remove query from fascet\n",
    "    for idx, el in enumerate(fascets):\n",
    "        fascets[idx] = fascets[idx].replace(query.lower(),\"\").strip()\n",
    "    \n",
    "    # using set() to remove duplicated from list \n",
    "    fascets = list(set(fascets))\n",
    "    \n",
    "    #remove empty element\n",
    "    fascets = list(filter(None, fascets))\n",
    "\n",
    "    return fascets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79010fd",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4521b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineApi:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        self.cors = CORS(self.app)\n",
    "        \n",
    "    def build_endpoints(self):\n",
    "        #Init Websearch unit\n",
    "        WSFramework = Quin()\n",
    "        #Init Fascets Extraction\n",
    "        extractor = Faspect()\n",
    "\n",
    "        @self.app.route(\"/extract\", methods=[\"GET\", \"POST\"])\n",
    "        @cross_origin()\n",
    "        def search_endpoint():\n",
    "            params = request.json\n",
    "            query = params[\"query\"]\n",
    "            \n",
    "            #Loadbalancing, if multiple requests only allow one at a time\n",
    "            if self.app.task_nextExec_time > datetime.now() : #if there is a waittime\n",
    "                print(\"Call '\" + query + \"' waitingtime: \" + str((self.app.task_nextExec_time - datetime.now()).total_seconds()))\n",
    "                timeToWait = (self.app.task_nextExec_time - datetime.now()).total_seconds() #calc waittime\n",
    "                self.app.task_nextExec_time = self.app.task_nextExec_time + timedelta(seconds=13) #increase next execution time by 10s\n",
    "                time.sleep(timeToWait)\n",
    "            else: #If there was no waittime, set next execution time to in 10s\n",
    "                self.app.task_nextExec_time = datetime.now() + timedelta(seconds=13) \n",
    "\n",
    "            print(\"Call '\" + query + \"' is now processing...\" )\n",
    "\n",
    "            #If more then 1 request per 13 second duckduckgo freezes -> open website in browser and search something!!\n",
    "            res = WSFramework.search_web_evidence(query)\n",
    "            if len(res) == 0:\n",
    "                #if there are too many requests, the connection is blocked. To unblock it usually helps to wait a littel and/or send a proper searchrequest\n",
    "                webbrowser.open_new('https://duckduckgo.com/?q=this+is+errorhandling.+Please+just+close+that+tab+and+have+a+great+day!&t=h_&ia=web')\n",
    "                res = WSFramework.search_web_evidence(query)              \n",
    "                \n",
    "            #Response to Snippetslist conversion\n",
    "            snippets = constructSnippets(res)\n",
    "            #Run Fascet Extraction and fascet preprocessing\n",
    "            results = []\n",
    "            for snippet in snippets: \n",
    "                resRaw = extractor.extract_facets(query, snippet, classification_threshold=0.05)\n",
    "                results.extend(preprocessingFascets(resRaw, query))\n",
    "            resultsFinal = preprocessingFascets(results, query)\n",
    "            \n",
    "            #Generate clarifying question\n",
    "            seed_line = query + \" | \" + (' , '.join(resultsFinal[0:fascetCount4Generation]))\n",
    "            clariQ = generate_prediction(seed_line = seed_line, model_ = model_loaded)\n",
    "            \n",
    "            #Extract best Fascets\n",
    "            Fascets = getBestFascets(clariQ[0], resultsFinal, fascetThreshold)\n",
    "            \n",
    "            #Construct Response\n",
    "            response_dict = {\n",
    "                \"query\": query,\n",
    "                \"snippets\": snippets,\n",
    "                \"fascets\": Fascets[1],\n",
    "                \"bestFascets\": Fascets[0],\n",
    "                \"clariQ\": clariQ\n",
    "            }\n",
    "            \n",
    "            response = json.dumps(response_dict)\n",
    "            print(\"Call '\" + query + \"' finished!\" )\n",
    "            return response\n",
    "\n",
    "    def serve(self, port=80):\n",
    "        self.build_endpoints()\n",
    "        self.app.task_nextExec_time = datetime.now()\n",
    "        self.app.run(host='0.0.0.0', port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7da53",
   "metadata": {},
   "source": [
    "### Start API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2670327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PipelineApi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DFINGE~1\\AppData\\Local\\Temp/ipykernel_9332/3555513268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPipelineApi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipelineApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mPipelineApi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7789\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PipelineApi' is not defined"
     ]
    }
   ],
   "source": [
    "PipelineApi = PipelineApi()\n",
    "PipelineApi.serve(port=7789)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
